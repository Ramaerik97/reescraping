#!/usr/bin/env python3
"""
Contoh Penggunaan Reescraping

File ini berisi berbagai contoh cara menggunakan WebScraper
untuk scraping website dengan berbagai skenario.

Author: Ramaerik97
"""

from web_scraper import WebScraper
import time


def example_basic_scraping():
    """
    Contoh dasar scraping website
    """
    print("\n" + "="*50)
    print("CONTOH 1: Basic Website Scraping")
    print("="*50)
    
    # Inisialisasi scraper
    scraper = WebScraper(timeout=30, delay=1)
    
    # URL yang akan di-scrape
    url = "https://httpbin.org/html"
    
    # Lakukan scraping
    result = scraper.scrape_website(url)
    
    if result:
        print(f"\nâœ… Scraping berhasil!")
        print(f"ğŸ“ File disimpan: {result['filepath']}")
        print(f"ğŸ“„ Panjang HTML: {result['html_length']} karakter")
        print(f"ğŸ¨ Jumlah CSS: {result['css_count']} file/block")
    else:
        print("âŒ Scraping gagal!")


def example_multiple_websites():
    """
    Contoh scraping beberapa website sekaligus
    """
    print("\n" + "="*50)
    print("CONTOH 2: Multiple Websites Scraping")
    print("="*50)
    
    # Daftar website yang akan di-scrape
    websites = [
        "https://httpbin.org/html",
        "https://example.com",
        "https://httpbin.org/"
    ]
    
    # Inisialisasi scraper dengan delay lebih lama untuk menghindari rate limiting
    scraper = WebScraper(timeout=30, delay=2)
    
    results = []
    
    for i, url in enumerate(websites, 1):
        print(f"\nğŸ”„ Scraping website {i}/{len(websites)}: {url}")
        
        result = scraper.scrape_website(url)
        
        if result:
            results.append(result)
            print(f"âœ… Berhasil: {result['filepath']}")
        else:
            print(f"âŒ Gagal scraping: {url}")
        
        # Delay antar website
        if i < len(websites):
            print("â³ Menunggu sebelum scraping berikutnya...")
            time.sleep(3)
    
    print(f"\nğŸ“Š Ringkasan: {len(results)}/{len(websites)} website berhasil di-scrape")
    
    return results


def example_custom_output_directory():
    """
    Contoh scraping dengan custom output directory
    """
    print("\n" + "="*50)
    print("CONTOH 3: Custom Output Directory")
    print("="*50)
    
    scraper = WebScraper()
    
    # Scraping dengan custom directory
    custom_dir = "scraping_results"
    url = "https://httpbin.org/html"
    
    print(f"ğŸ“ Menyimpan hasil ke directory: {custom_dir}")
    
    result = scraper.scrape_website(url, output_dir=custom_dir)
    
    if result:
        print(f"âœ… File disimpan di: {result['filepath']}")
    else:
        print("âŒ Scraping gagal!")


def example_manual_scraping():
    """
    Contoh penggunaan manual (step by step)
    """
    print("\n" + "="*50)
    print("CONTOH 4: Manual Step-by-Step Scraping")
    print("="*50)
    
    scraper = WebScraper()
    url = "https://httpbin.org/html"
    
    print("ğŸ”„ Step 1: Mengambil HTML content...")
    html_content, soup, status_code = scraper.get_html_content(url)
    
    if not html_content:
        print("âŒ Gagal mengambil HTML")
        return
    
    print(f"âœ… HTML berhasil diambil (Status: {status_code})")
    print(f"ğŸ“„ Panjang HTML: {len(html_content)} karakter")
    
    print("\nğŸ”„ Step 2: Mengekstrak CSS...")
    css_data = scraper.extract_css(soup, url)
    print(f"ğŸ¨ Internal CSS: {len(css_data['internal_css'])} block")
    print(f"ğŸ¨ External CSS: {len(css_data['external_css'])} file")
    
    print("\nğŸ”„ Step 3: Mengekstrak metadata...")
    metadata = scraper.extract_metadata(soup)
    print(f"ğŸ“‹ Title: {metadata.get('title', 'N/A')}")
    print(f"ğŸ“‹ Description: {metadata.get('description', 'N/A')[:100]}...")
    
    print("\nğŸ”„ Step 4: Menyimpan ke file...")
    filepath = scraper.save_to_markdown(url, html_content, css_data, metadata)
    
    if filepath:
        print(f"âœ… File disimpan: {filepath}")
    else:
        print("âŒ Gagal menyimpan file")


def example_error_handling():
    """
    Contoh handling error untuk URL yang tidak valid
    """
    print("\n" + "="*50)
    print("CONTOH 5: Error Handling")
    print("="*50)
    
    scraper = WebScraper(timeout=10)
    
    # URL yang tidak valid atau tidak bisa diakses
    invalid_urls = [
        "https://website-yang-tidak-ada.com",
        "http://localhost:99999",
        "invalid-url"
    ]
    
    for url in invalid_urls:
        print(f"\nğŸ”„ Mencoba scraping: {url}")
        result = scraper.scrape_website(url)
        
        if result:
            print(f"âœ… Berhasil: {result['filepath']}")
        else:
            print(f"âŒ Gagal scraping URL: {url}")


def main():
    """
    Fungsi utama untuk menjalankan semua contoh
    """
    print("ğŸš€ Reescraping - Contoh Penggunaan")
    print("========================================")
    
    try:
        # Jalankan contoh-contoh
        example_basic_scraping()
        
        input("\nâ¸ï¸  Tekan Enter untuk melanjutkan ke contoh berikutnya...")
        example_multiple_websites()
        
        input("\nâ¸ï¸  Tekan Enter untuk melanjutkan ke contoh berikutnya...")
        example_custom_output_directory()
        
        input("\nâ¸ï¸  Tekan Enter untuk melanjutkan ke contoh berikutnya...")
        example_manual_scraping()
        
        input("\nâ¸ï¸  Tekan Enter untuk melanjutkan ke contoh berikutnya...")
        example_error_handling()
        
        print("\nğŸ‰ Semua contoh selesai dijalankan!")
        print("ğŸ“ Cek folder 'hasil' dan 'scraping_results' untuk melihat file hasil scraping.")
        
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  Program dihentikan oleh user.")
    except Exception as e:
        print(f"\nâŒ Error: {e}")


if __name__ == "__main__":
    main()